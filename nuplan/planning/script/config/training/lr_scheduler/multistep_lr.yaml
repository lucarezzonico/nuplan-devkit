_target_: torch.optim.lr_scheduler.MultiStepLR
_convert_: all

milestones: [10, 20, 30, 40, 50]  # decays the learning rate of each parameter group by gamma once the number of epochs equals one of the milestones
gamma: 0.1  # multiplicative factor of learning rate decay
