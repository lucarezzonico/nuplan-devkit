_target_: torch.optim.lr_scheduler.MultiStepLR
_convert_: all

milestones: [3, 6, 9, 12, 15]  # decays the learning rate of each parameter group by gamma once the number of epochs equals one of the milestones
gamma: 0.5  # multiplicative factor of learning rate decay
